Namespace(epoch=40000, imgc=1, imgsz=28, k_qry=15, k_spt=1, meta_lr=0.001, n_way=5, task_num=32, update_lr=0.4, update_step=5, update_step_test=10)
Meta(
  (net): Learner(
    conv2d:(ch_in:1, ch_out:64, k:3x3, stride:2, padding:0)
    relu:(True,)
    bn:(64,)
    conv2d:(ch_in:64, ch_out:64, k:3x3, stride:2, padding:0)
    relu:(True,)
    bn:(64,)
    conv2d:(ch_in:64, ch_out:64, k:3x3, stride:2, padding:0)
    relu:(True,)
    bn:(64,)
    conv2d:(ch_in:64, ch_out:64, k:2x2, stride:1, padding:0)
    relu:(True,)
    bn:(64,)
    flatten:()
    linear:(in:64, out:5)

    (vars): ParameterList(
        (0): Parameter containing: [torch.cuda.FloatTensor of size 64x1x3x3 (GPU 0)]
        (1): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
        (2): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
        (3): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
        (4): Parameter containing: [torch.cuda.FloatTensor of size 64x64x3x3 (GPU 0)]
        (5): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
        (6): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
        (7): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
        (8): Parameter containing: [torch.cuda.FloatTensor of size 64x64x3x3 (GPU 0)]
        (9): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
        (10): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
        (11): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
        (12): Parameter containing: [torch.cuda.FloatTensor of size 64x64x2x2 (GPU 0)]
        (13): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
        (14): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
        (15): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
        (16): Parameter containing: [torch.cuda.FloatTensor of size 5x64 (GPU 0)]
        (17): Parameter containing: [torch.cuda.FloatTensor of size 5 (GPU 0)]
    )
    (vars_bn): ParameterList(
        (0): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
        (1): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
        (2): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
        (3): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
        (4): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
        (5): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
        (6): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
        (7): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
    )
  )
)
Total trainable tensors: 91781
load from omniglot.npy.
DB: train (1200, 20, 1, 28, 28) test (423, 20, 1, 28, 28)
omniglot_train.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  y_spt = torch.tensor(y_spt, dtype=torch.int64, device=device) # diff syntax ?
omniglot_train.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  y_qry = torch.tensor(y_qry, dtype=torch.int64, device=device)
step: 0         training acc: [0.20375    0.31708333 0.38958333 0.39916667 0.39916667 0.40166667]
omniglot_train.py:76: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  y_spt = torch.tensor(y_spt, dtype=torch.int64, device=device) # diff syntax ?
omniglot_train.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  y_qry = torch.tensor(y_qry, dtype=torch.int64, device=device)
Test acc: [0.2007 0.3015 0.3804 0.3926 0.3945 0.3958 0.3965 0.3972 0.3977 0.3982
 0.3987]
step: 4000      training acc: [0.19416667 0.33708333 0.41625    0.43       0.43458333 0.43541667]
step: 8000      training acc: [0.20125    0.29083333 0.39458333 0.40541667 0.40666667 0.40666667]
step: 12000     training acc: [0.20833333 0.32166667 0.43666667 0.44083333 0.4425     0.44333333]
Test acc: [0.2043 0.3066 0.393  0.4055 0.4082 0.409  0.4097 0.4102 0.4106 0.411
 0.4116]
step: 16000     training acc: [0.21208333 0.3475     0.42458333 0.43833333 0.44041667 0.44125   ]
step: 20000     training acc: [0.17333333 0.31125    0.41333333 0.42625    0.42833333 0.42958333]
step: 24000     training acc: [0.20125    0.34083333 0.48166667 0.495      0.49666667 0.49791667]
Test acc: [0.1987 0.3186 0.4045 0.4185 0.4202 0.421  0.4214 0.422  0.4226 0.423
 0.4236]
step: 28000     training acc: [0.21958333 0.33875    0.435      0.445      0.4475     0.44875   ]
step: 32000     training acc: [0.19416667 0.33791667 0.41625    0.43125    0.43291667 0.43541667]
step: 36000     training acc: [0.20208333 0.3425     0.4625     0.46583333 0.46833333 0.47      ]
Test acc: [0.2004 0.3315 0.4233 0.4355 0.4375 0.439  0.4397 0.4404 0.4407 0.4414
 0.442 ]